{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> #### 在机器学习里，通常来说我们不能将全部用于数据训练模型，否则我们将没有数据集对该模型进行验证，从而评估我们的模型的预测效果。为了解决这一问题，有如下常用的方法（既是数据处理方法也是评估方法）：","metadata":{}},{"cell_type":"markdown","source":"----\n- #### 1. The Validation Set Approach","metadata":{}},{"cell_type":"markdown","source":"#### 最简单的做法，将数据划分成训练集和测试集\n\n#### 弊端：①无法用完全部数据集来训练模型 ②不同的训练集划分会对模型可能造成不小影响","metadata":{}},{"cell_type":"markdown","source":"----\n- #### 2. Cross-Validation（交叉验证）","metadata":{}},{"cell_type":"markdown","source":"- - #### 2.1 LOOCV\n#### Leave-one-out cross-validation， 将数据集划分成n份， 执行n次，每次取其中一份作为测试集，其余为训练集，我们最终训练了n个模型，每次都能得到一个MSE。而计算最终test MSE则就是将这n个MSE取平均。\n#### 弊端: 计算量过于大，是test set approach耗时的n-1倍。但某些模型可以通过公式化简成通解公式（岭回归）","metadata":{}},{"cell_type":"markdown","source":"- - #### 2.2 K-fold Cross Validation\n#### data: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],K 取 3得到以下三个Fold\n#### Fold1: [0.5, 0.2]\n#### Fold2: [0.1, 0.3]\n#### Fold3: [0.4, 0.6]\n#### Model1: Trained on Fold1 + Fold2, Tested on Fold3\n#### Model2: Trained on Fold2 + Fold3, Tested on Fold1\n#### Model3: Trained on Fold1 + Fold3, Tested on Fold2\n#### **sklearn中有内置函数接口**","metadata":{}},{"cell_type":"markdown","source":"> #### 参考https://zhuanlan.zhihu.com/p/24825503\n> #### 参考https://zhuanlan.zhihu.com/p/607797639\n> #### 参考https://blog.csdn.net/sdssee/article/details/88368020","metadata":{}}]}