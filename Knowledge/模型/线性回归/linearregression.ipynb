{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 线性回归\n> #### 在标准线性回归中，通过最小化真实值($y_i$)和预测值($\\stackrel{\\mathrm{ˆ}}{y_i}$)的平方误差来训练模型，即残差平方和($RSS, Residual Sum Of Squares$)\n> #### $RSS$ = $\\mathop{∑}\\limits_{i=1}^n$($y_i - \\stackrel{\\mathrm{ˆ}}{y_i}$)","metadata":{}},{"cell_type":"markdown","source":"## 最小二乘法","metadata":{}},{"cell_type":"markdown","source":"----\n- #### 代数推导\n","metadata":{}},{"cell_type":"markdown","source":"#### RSS 设为 $f(x) = $$\\mathop{∑}\\limits_{i=1}^n$$(y_i - (ω_0 + ω_1x_i))^2$\n\n#### 对$ω_0$和$ω_1$求偏导得\n\n####  $\\frac{δf}{δω_0} = -2∑(y_i) - nω_0 - ∑(ω_1x_i)$\n\n####  $\\frac{δf}{δω_1} = -2∑(x_iy_i - ω_0x_i + ω_1x_i^2)$\n\n#### 分别令偏导为$0$得\n\n#### $ω_1 = \\frac{n∑(x_iy_i) - ∑(x_i)∑(y_i)}{n∑(x_i^2) - (∑x_i)^2}$\n\n#### $ω_0 = \\frac{∑(x_i^2)∑(y_i) - ∑(x_i)∑(x_iy_i)}{n∑(x_i^2) - (∑x_i)^2}$\n\n#### 所求参数即为最佳拟合曲线参数","metadata":{}},{"cell_type":"markdown","source":"----\n- #### 矩阵推导","metadata":{}},{"cell_type":"markdown","source":"#### RSS 设为 $f(x) = $$\\mathop{∑}\\limits_{i=1}^n$$(y_i - (ω_0 + ω_1x_i))^2 = (y - XW)^T(y - XW) = y^Ty - 2(XW)^Ty + (XW)^T(XW)$\n\n#### 对矩阵求偏导\n\n#### 第一项为常数项直接省去\n\n#### 第二项: $\\frac{δ(-2(XW)^Ty)}{δW} = -2X^Ty$\n\n#### 第三项: $\\frac{δ((XW)^T(XW))}{δW} = 2\\frac{δ(W^TX^T)}{δW}WX$\n\n#### 令偏导为$0$得\n\n#### $W = (X^TX)^{-1}X^Ty$","metadata":{}},{"cell_type":"markdown","source":"----\n- #### 常数项处理\n","metadata":{}},{"cell_type":"markdown","source":"#### 需要注意代数推导中$ω_0$在矩阵形式中，相当于添加了一个特征值$x_0$且$x_0$恒为$1$，也就是目标函数中的$ω_0$可以看作$ω_0*x_0$形成常数项\n\n#### 即：\n\n#### $ω_0 + ω_1x_{11} + ω_2x_{12} + ω_3x_{13} + ... + ω_nx_{1n} = y_1$\n#### $ω_0 + ω_1x_{21} + ω_2x_{22} + ω_3x_{23} + ... + ω_nx_{2n} = y_2$\n#### ...\n#### $ω_0 + ω_1x_{m1} + ω_2x_{m2} + ω_3x_{m3} + ... + ω_nx_{mn} = y_m$\n\n#### 矩阵表示为：\n\n#### $$\n\\begin{bmatrix}\n1&x_{11}&x_{12}&x_{13}&…&x_{1n}\\\\\n1&x_{21}&x_{22}&x_{23}&…&x_{2n}\\\\\n…&…&…&…&…&…\\\\\n1&x_{m1}&x_{m2}&x_{m3}&…&x_{mn}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nω_0\\\\\nω_1\\\\\nω_2\\\\\nω_3\\\\\n…\\\\\nω_n\\\\\n\\end{bmatrix}\n = \n\\begin{bmatrix}\ny_1\\\\\ny_2\\\\\n…\\\\\ny_m\\\\\n\\end{bmatrix}\n$$\n\n#### 即： $Aω = Y$\n","metadata":{}},{"cell_type":"markdown","source":"----\n- #### sklearn使用","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression as LR  # 线性回归\nimport pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 建模\nreg = LR().fit(xtrain,ytrain)\nytest = reg.predict(xtest)\nreg.coef_ # 回归系数","metadata":{},"execution_count":null,"outputs":[]}]}