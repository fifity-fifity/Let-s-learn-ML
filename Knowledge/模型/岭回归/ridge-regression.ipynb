{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> ### 岭回归是对最小二乘法的优化","metadata":{}},{"cell_type":"markdown","source":"#### 首先最小二乘总是寻找拟合残差与特征正交的方向，但由于数据中存在噪声，按照最小二乘必须要满足正交的关系，可能会使得求解系数过大，导致模型过拟合。为了解决这个问题，岭回归加入对参数的惩罚项，通过L2范数和惩罚系数来约束参数的大小。\n\n#### 同时也可以通过惩罚系数间接解决特征之间构成的矩阵不可逆问题","metadata":{}},{"cell_type":"markdown","source":"----\n- #### 数学推导","metadata":{}},{"cell_type":"markdown","source":"#### $Loss = RSS + λω^Tω = (y - Xω)^T(y - Xω) + λω^Tω$\n##### 化简后:\n#### $Loss = y^Ty - y^TXω - ω^TX^Ty + ωX^TXω + λω^Tω$\n##### 对$ω$求偏导:\n#### $\\frac{∂Loss}{∂ω} = -X^Ty - X^Ty + 2X^TXω + 2λω$\n##### 令偏导为0，解得:\n#### $ω = (X^TX + λI)^{-1}X^Ty$  (其中 $I$ 为单位矩阵) ","metadata":{}},{"cell_type":"markdown","source":"#### 对于惩罚系数 $λ$ 通常会使用交叉验证搜索得到，这里采用 $Loocv$ ，对于岭回归中 $Loocv$ 存在通解公式，以下为公式推导：","metadata":{}},{"cell_type":"markdown","source":"- ##### 以 $-i$ 为下标的变量表示去掉第 $i$ 个样本后的数据\n- ##### $X$ 表示特征矩阵， $x_i$ 表示 $X$ 的第 $i$ 行向量的转置，$y$ 表示标签向量， $y_i$ 为该向量的第 $i$ 个元素（标量）\n","metadata":{}},{"cell_type":"markdown","source":"#### $step 0$\n#### 简略描述一下 $Loocv$ 过程为：假定一个 $λ$ 后，将划分好的 $n$ 个样本集， $n$ 次分成 $n-1$ 个训练集，$1$ 个验证集，训练出 $n$ 模型， 算出平均模型残差以表示 $loss$ 评分， 即：\n#### $loocv\\_loss = \\frac{1}{n}∑_{i = 1}^n || y_i - x_i^Tω_{-i,λ} ||^2$\n#### $choose\\_λ = min_{loocv\\_loss}λ$","metadata":{}},{"cell_type":"markdown","source":"#### $step 1$\n#### $ω_{-i,λ} = (X_{-i}^TX_{-i} + λI)^{-1}X^T_{-i}y_{-i}$\n#### $ω_{-i,λ} = (X^TX - x_ix_i^T + λI)^{-1}(X^Ty - x_iy_i)$\n\n- #### 为了进一步展开用到 $Sherman-Morrison$公式: $(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}$\n- #### 令 $X^TX + λI = A$ \n\n#### $ω_{-i,λ} = \\left(A^{-1}+\\frac{A^{-1} x_{i} x_{i}^{T} A^{-1}}{1-x_{i}^{T} A^{-1} x_{i}}\\right)\\left(X^{T} y-x_{i} y_{i}\\right)$\n\n#### $ω_{-i,λ} =A^{-1} X^{T} y-A^{-1} x_{i} y_{i}+\\frac{A^{-1} x_{i} x_{i}^{T} A^{-1}}{1-x_{i}^{T} A^{-1} x_{i}}\\left(X^{T} y-x_{i} y_{i}\\right)$\n\n","metadata":{}},{"cell_type":"markdown","source":"#### $step 2$\n#### 可以发现 $A^{-1} X^{T} y$ 即为 $ω_λ$, 再定义 $x_i^T A_{-1}x_i = h_i$， 则有\n\n#### $\\begin{align}\nω_{-i,λ} & = ω_λ-A^{-1} x_{i} y_{i}+\\frac{A^{-1} x_{i} \\hat{y}_{i}-A^{-1} x_{i} h_{i} y_{i}}{1-h_{i}} \\\\\n& =ω_λ+\\frac{-A^{-1} x_{i} y_{i}+A^{-1} x_{i} h_{i} y_{i}-A^{-1} x_{i} h_{i} y_{i}+A^{-1} x_{i} \\hat{y}_{i}}{1-h_{i}} \\\\\n& =ω_i+\\frac{A^{-1} x_{i}\\left(\\hat{y}_{i}-y_{i}\\right)}{1-h_{i}}\n\\end{align}$\n\n#### $\\begin{align}\ny_{i}-x_{i}^{T} ω_{-i, \\lambda} & =y_{i}-x_{i}^{T}\\left(ω+\\frac{A^{-1} x_{i}}{1-h_{i}}\\left(\\hat{y}_{i}-y_{i}\\right)\\right) \\\\\n& =y_{i}-\\hat{y}_{i}-\\frac{h_{i}}{1-h_{i}}\\left(\\hat{y}_{i}-y_{i}\\right) \\\\\n& =\\left(y_{i}-\\hat{y}_{i}\\right)\\left(1+\\frac{h_{i}}{1-h_{i}}\\right) \\\\\n& =\\frac{y_{i}-\\hat{y}_{i}}{1-h_{i}}\n\\end{align}$","metadata":{}},{"cell_type":"markdown","source":"#### 故综上可得：\n#### $loocv\\_loss=\\frac{1}{n} \\sum_{i=1}^{n}\\left|\\frac{y_{i}-\\hat{y}_{i}}{1-h_{i}}\\right|^{2} $\n#### $h_{i}=x_{i}^{T} A^{-1} x_{i}, \\quad A=X^{T} X+\\lambda I$\n\n#### 即对于每次选定的 $λ$ 后没必要训练 $n$ 个模型求损失函数， 可以直接计算该 $λ$ 下的预测值即可直接求出 $loss$","metadata":{}},{"cell_type":"markdown","source":"----\n- #### sklearn 中模型使用","metadata":{}},{"cell_type":"markdown","source":"- ##### 不使用交叉检验的Riage模型： linear_model.Ridge\n- ##### 使用交叉检验的Riage模型 linear_model.RidgeCV\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import mean_squared_error,r2_score\n\n#定义样本和特征数量\nnum_sample=5000\nnum_feature=50\nweight=-3.4\nb_true=4.3\nfeature=np.random.normal(size=(num_sample,num_feature))\nlabel=weight*feature+b_true+np.random.normal(size=(num_sample,num_feature))\n\n# Split the data into training/testing sets\nX_train = feature[:-100,:]\nX_test = feature[-100:,:]\n\n# Split the targets into training/testing sets\ny_train = label[:-100]\ny_test = label[-100:]\n\n#创建Riage模型\nreg=linear_model.Ridge(alpha=0.5,fit_intercept=True)\nregCV=linear_model.RidgeCV(alphas=[0.1,0.3,0.5,0.7,1, 2, 3, 4, 5],fit_intercept=True)\n#传入训练集训练模型\nreg.fit(X_train,y_train)\nregCV.fit(X_train,y_train)\n#传入测试集得到结果\ny_predict=reg.predict(X_test)\ny_CV_predict=reg.predict(X_test)\n#求解均方误差\n# print(y_predict, y_CV_predict)\nprint(mean_squared_error(y_test,y_predict))\nprint(mean_squared_error(y_test,y_CV_predict))\nprint('RidgeCV最优的alpha_: ',regCV.alpha_)\n# #求解R^2,最大值为1，越接近1，模型越完美\nprint(r2_score(y_test, y_predict))\nprint(r2_score(y_test, y_CV_predict))\n# print(\"Coefficient of the model:%.2f\"%reg.coef_)\n# print(\"intercept of the model:%.2f\"%reg.intercept_)\n\n# ax = plt.subplot(111)\n# ax.scatter(X_test,y_test)\n# ax.plot(X_test,y_predict)\n\n# plt.show()\n\n# 不知道为什么最优RidgeCV的alpha_与Ridge不同但回归效果相同","metadata":{"execution":{"iopub.status.busy":"2023-03-01T14:15:36.353224Z","iopub.execute_input":"2023-03-01T14:15:36.353694Z","iopub.status.idle":"2023-03-01T14:15:37.147268Z","shell.execute_reply.started":"2023-03-01T14:15:36.353640Z","shell.execute_reply":"2023-03-01T14:15:37.145967Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"1.0264043578139008\n1.0264043578139008\nRidgeCV最优的alpha_:  4.0\n0.9172936847296822\n0.9172936847296822\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> #### 参考https://zhuanlan.zhihu.com/p/607797639\n> #### 参考https://zhuanlan.zhihu.com/p/165493873","metadata":{}}]}